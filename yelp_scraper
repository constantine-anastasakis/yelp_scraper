# Use this script to collect and organize information from Yelp.com

# The script requires the user to install Beautiful Soup, a Python package for parsing HTML and XML documents - documentation (including installation information) avaiable here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

import datetime
import requests
from bs4 import BeautifulSoup
import os
import csv

csv_headers = ["business_name", "neighborhood", "address", "phone"]

# The user must create a folder titled "scrapings" on their desktop.

def write_products_to_file(filename="listings.csv", products=[]):
    filepath = os.path.join(os.path.dirname(__file__), "scrapings", filename)
    with open(filepath, "w") as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=csv_headers)
        writer.writeheader()
        for product in products:
            writer.writerow(product)

products = []

# The default starting URL in this script shows the first page of listings for "Restaurants" in "New York." User can swap the URL with the URL of the first page of search results for any search query.

# The starting URL will only provide the first 30 search listings, the script must be able to crawl ALL listings.

search_results = range(0,990,30) 
for results in search_results:
    r = requests.get(f"https://www.yelp.com/search?find_desc=Restaurants&find_loc=New+York,+NY&start={results}")

    raw = r.text
    soup = BeautifulSoup(raw, "html.parser")

    listings = soup.select(selector=".regular-search-result")

    for listing in listings:
        business_name = listing.find("a",{"class":"biz-name"})
        neighborhood = listing.find("span",{"class":"neighborhood-str-list"})
        address = listing.find("address")
        phone = listing.find("span",{"class":"biz-phone"})

        d = {"business_name":business_name.text.strip(),"neighborhood":neighborhood.text.strip(),"address":address.text.strip(),"phone":phone.text.strip()}

        products.append(d)

        print(d)

        write_products_to_file(products=products)
